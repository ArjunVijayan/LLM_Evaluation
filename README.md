This repository provides a modular evaluation suite for assessing the performance of large language models (LLMs) across multiple key metrics, including Faithfulness, Relevancy, Contextual Accuracy, Completeness, and Latency. Each metric is implemented as a standalone class, allowing for easy integration and testing of LLM-generated outputs.

